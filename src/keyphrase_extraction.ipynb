{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'ok. automatic keyphrase extraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileFPath='/home/mprakash/Projects/enron-experiments/significant-terms/output/firstpass/maildir/delainey-d/maildir_delainey-d_all_documents_72.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Content\n",
      "Dorie, I have to question the effectiveness of this program.  Over the years, \n",
      "it appears that more spots are being filled by Enron people then customers. \n",
      "The effectiveness of this forum from a customer point of view is \n",
      "questionable.  I vote to kill it.\n",
      "\n",
      "Please forgive the approach - but I would like to provoke a discussion with \n",
      "my partners.\n",
      "\n",
      "Regards\n",
      "Delainey\n",
      "\t\n",
      "\t\n",
      "\tFrom:  Dorie Hitchcock                           11/30/2000 06:40 AM\n",
      "\t\n",
      "\n",
      "To: Sylvia S Pollan/HOU/ECT@ECT, Robyn Zivic/NA/Enron@Enron, Andrew H \n",
      "Lewis/HOU/ECT@ECT, Geoff Storey/HOU/ECT@ECT, Martin Cuilla/HOU/ECT@ECT, \n",
      "Patrice L Mims/HOU/ECT@ECT, Kevin Ruscitti/HOU/ECT@ECT, Kelli \n",
      "Stevens/HOU/ECT@ECT, Tom Donohoe/HOU/ECT@ECT, Joe Parks/Corp/Enron@ENRON, \n",
      "Janet H Wallis/HOU/ECT@ECT, Gary Bryan/HOU/ECT@ECT, Jill T \n",
      "Zivley/HOU/ECT@ECT, Brian Bierbach/NA/Enron@Enron, John Craig \n",
      "Taylor/HOU/ECT@ECT, Gary Hickerson/HOU/ECT@ECT, Daniel Reck/HOU/ECT@ECT, \n",
      "George McClellan/HOU/ECT@ECT, Mark Tawney/HOU/ECT@ECT, Jere C \n",
      "Overdyke/HOU/ECT@ECT, Larry Lawyer/NA/Enron@Enron, Brent A Price/HOU/ECT@ECT, \n",
      "Eric Gonzales/LON/ECT@ECT, Cindy Skinner/HOU/ECT@ECT, John L \n",
      "Nowlan/HOU/ECT@ECT, Don Schroeder/HOU/ECT@ECT, Doug Leach/HOU/ECT@ECT, Bryan \n",
      "Burnett/HOU/ECT@ECT, Wendy King/Corp/Enron@ENRON, Tim Battaglia@Enron, \n",
      "Douglas B Dunn/HOU/ECT@ECT, Rodney Malcolm/HOU/ECT@ECT, Billy \n",
      "Lemmons/Corp/Enron@ENRON, David Howe/Corp/Enron@ENRON, Mark \n",
      "Frevert/NA/Enron@Enron, John J Lavorato/Corp/Enron@Enron, David W \n",
      "Delainey/HOU/ECT@ECT, Mike McConnell/HOU/ECT@ECT, Jeffrey A \n",
      "Shankman/HOU/ECT@ECT, Jeffrey McMahon/HOU/ECT@ECT, Raymond Bowen/HOU/ECT@ECT, \n",
      "Jeffery Ader/HOU/ECT@ECT, Phillip K Allen/HOU/ECT@ECT, John \n",
      "Arnold/HOU/ECT@ECT, Edward D Baughman/HOU/ECT@ECT, Sally Beck/HOU/ECT@ECT, \n",
      "Tim Belden/HOU/ECT@ECT, Christopher F Calger/PDX/ECT@ECT, Wes \n",
      "Colwell/HOU/ECT@ECT, Derek Davies/CAL/ECT@ECT, Mark Dana Davis/HOU/ECT@ECT, \n",
      "Joseph Deffner/HOU/ECT@ECT, Paul Devries/TOR/ECT@ECT, Janet R \n",
      "Dietrich/HOU/ECT@ECT, Jeff Donahue/HOU/ECT@ECT, Stephen H \n",
      "Douglas/HOU/ECT@ECT, W David Duran/HOU/ECT@ECT, Chris H Foster/HOU/ECT@ECT, \n",
      "Mark E Haedicke/HOU/ECT@ECT, Rogers Herndon/HOU/ECT@ect, Scott \n",
      "Josey/Corp/Enron@ENRON, C John Thompson/Corp/Enron@ENRON, Fred \n",
      "Lagrasta/HOU/ECT@ECT, Eric LeDain/CAL/ECT@ECT, Laura Luce/Corp/Enron@Enron, \n",
      "Thomas A Martin/HOU/ECT@ECT, Jonathan McKay/CAL/ECT@ECT, Michael L \n",
      "Miller/NA/Enron@Enron, Rob Milnthorp/CAL/ECT@ECT, Jean Mrha/NA/Enron@Enron, \n",
      "Scott Neal/HOU/ECT@ECT, David Oxley/HOU/ECT@ECT, Ozzie Pagan/HOU/ECT@ECT, \n",
      "Beth Perlman/HOU/ECT@ECT, Kevin M Presto/HOU/ECT@ECT, Brian \n",
      "Redmond/HOU/ECT@ECT, Hunter S Shively/HOU/ECT@ECT, James D \n",
      "Steffes/NA/Enron@Enron, Fletcher J Sturm/HOU/ECT@ECT, Bruce \n",
      "Sukaly/Corp/Enron@Enron, Mike Swerzbin/HOU/ECT@ECT, Scott \n",
      "Tholan/Corp/Enron@Enron, Barry Tycholiz/CAL/ECT@ECT, Frank W \n",
      "Vickers/HOU/ECT@ECT, Greg Wolfe/HOU/ECT@ECT, Max Yzaguirre/NA/Enron@ENRON, \n",
      "John Zufferli/CAL/ECT@ECT, Deirdre McCaffrey/HOU/ECT@ECT, Jennifer \n",
      "Shipos/HOU/ECT@ECT\n",
      "cc:  \n",
      "Subject: 2001 ENA/EGM/EIM CUSTOMER SKI PROGRAM\n",
      "\n",
      "Winter is just around the corner and the new ski season will be opening \n",
      "soon.  Please review the proposed information below, regarding the \n",
      "ENA/EGM/EIM Customer Ski Program, to help us determine what our needs will be \n",
      "for the 2001 season.\n",
      "\n",
      "Location:  Beaver Creek, Colorado\n",
      "Preliminary Dates:  February 21 - March 14, 2001\n",
      "Approximate Cost Per Person:  $2,600 (based on 82 participants)\n",
      "\n",
      "The preliminary dates have been divided into 6 trips:\n",
      "TRIP\tDATES\tTOTAL # OF BEDS\n",
      "Trip 1\tWed, February 21 - Sat, February 24\t11\n",
      "Trip 2\tSun, February 25 - Wed, February 28\t17\n",
      "Trip 3\tWed, February 28 - Sat, March 3\t13\n",
      "Trip 4\tSun, March 4 - Wed, March 7\t13\n",
      "Trip 5\tWed, March 7 - Sat, March 10\t13\n",
      "Trip 6\tSun, March 11 - Wed, March 14\t15\n",
      "\n",
      "The program cost per person includes:\n",
      "? Four days/three nights accommodations in luxurious private homes in the \n",
      "premier Holden Road/Borders Road area of Beaver  Creek with daily maid service\n",
      "? Animated invitation with on-line registration\n",
      "? Round-trip airport ground transportation\n",
      "? Experienced property hosts\n",
      "? Daily lift tickets and ski instruction\n",
      "? Ski equipment rental\n",
      "? Catered dinner two nights\n",
      "? Catered breakfast daily\n",
      "? Off-site dinner coordination\n",
      "? Alternate activity coordination\n",
      "? Private vans at each house\n",
      "? One massage with experienced in-house massage therapists\n",
      "? Enron promotional gift item\n",
      "? Pre-program administration and coordination\n",
      "? On-site operation\n",
      "\n",
      "If you are interested in participating in the 2001 ski program, please submit \n",
      "an email  request to me no later than Friday, December 8th, including the \n",
      "following information:\n",
      "Name\n",
      "Company\n",
      "Department\n",
      "Telephone\n",
      "Fax\n",
      "Number of People\n",
      "Choice of Dates (1st, 2nd & 3rd)\n",
      "Company #\n",
      "RC #\n",
      "\n",
      "The week of December 11th, all requests will be compiled for review and final \n",
      "approval.\n",
      "\n",
      "Questions should be directed to my attention at (713) 853-6978.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(fileFPath, 'r')\n",
    "content = f.read()\n",
    "f.close()\n",
    "print 'File Content'\n",
    "print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Keyphrases\n",
      "the effectiveness\n",
      "[(u'the', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "------------------------------------------\n",
      "this program\n",
      "[(u'this', u'DET'), (u'program', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'program', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'this', u'DET'), (u'program', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'this', u'DET'), (u'program', u'NOUN')]\n",
      "------------------------------------------\n",
      "the year\n",
      "[(u'the', u'DET'), (u'years', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'years', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'years', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'years', u'NOUN')]\n",
      "------------------------------------------\n",
      "more spot\n",
      "[(u'more', u'ADJ'), (u'spots', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'more', u'ADJ'), (u'spots', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'more', u'ADJ'), (u'spots', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'more', u'ADJ'), (u'spots', u'NOUN')]\n",
      "------------------------------------------\n",
      "enron people\n",
      "[(u'Enron', u'PROPN'), (u'people', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'enron', u'PROPN'), (u'people', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'Enron', u'PROPN'), (u'people', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'enron', u'PROPN'), (u'people', u'NOUN')]\n",
      "------------------------------------------\n",
      "the effectiveness\n",
      "[(u'The', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'The', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'effectiveness', u'NOUN')]\n",
      "------------------------------------------\n",
      "this forum\n",
      "[(u'this', u'DET'), (u'forum', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'forum', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'this', u'DET'), (u'forum', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'this', u'DET'), (u'forum', u'NOUN')]\n",
      "------------------------------------------\n",
      "a customer point\n",
      "[(u'a', u'DET'), (u'customer', u'NOUN'), (u'point', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'customer', u'NOUN'), (u'point', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'a', u'DET'), (u'customer', u'NOUN'), (u'point', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'a', u'DET'), (u'customer', u'NOUN'), (u'point', u'NOUN')]\n",
      "------------------------------------------\n",
      "view\n",
      "[(u'view', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'view', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'view', u'NOUN')]\n",
      "------------------------------------------\n",
      "the approach\n",
      "[(u'the', u'DET'), (u'approach', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'approach', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'approach', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'approach', u'NOUN')]\n",
      "------------------------------------------\n",
      "a discussion\n",
      "[(u'a', u'DET'), (u'discussion', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'discussion', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'a', u'DET'), (u'discussion', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'a', u'DET'), (u'discussion', u'NOUN')]\n",
      "------------------------------------------\n",
      "my partner\n",
      "[(u'my', u'ADJ'), (u'partners', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'partners', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'my', u'ADJ'), (u'partners', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'my', u'ADJ'), (u'partners', u'NOUN')]\n",
      "------------------------------------------\n",
      "the corner\n",
      "[(u'the', u'DET'), (u'corner', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'corner', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'corner', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'corner', u'NOUN')]\n",
      "------------------------------------------\n",
      "the new ski season\n",
      "[(u'the', u'DET'), (u'new', u'ADJ'), (u'ski', u'NOUN'), (u'season', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'new', u'ADJ'), (u'ski', u'NOUN'), (u'season', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'new', u'ADJ'), (u'ski', u'NOUN'), (u'season', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'new', u'ADJ'), (u'ski', u'NOUN'), (u'season', u'NOUN')]\n",
      "------------------------------------------\n",
      "the propose information\n",
      "[(u'the', u'DET'), (u'proposed', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'proposed', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'proposed', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'proposed', u'VERB')]\n",
      "------------------------------------------\n",
      "what\n",
      "[(u'what', u'NOUN')]\n",
      "preprocess_np_drop_unwanted []\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'what', u'NOUN')]\n",
      "------------------------------------------\n",
      "our need\n",
      "[(u'our', u'ADJ'), (u'needs', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'our', u'ADJ'), (u'needs', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'our', u'ADJ'), (u'needs', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'our', u'ADJ'), (u'needs', u'NOUN')]\n",
      "------------------------------------------\n",
      "the 2001 season\n",
      "[(u'the', u'DET'), (u'2001', u'NUM'), (u'season', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'season', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'2001', u'NUM'), (u'season', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'2001', u'NUM'), (u'season', u'NOUN')]\n",
      "------------------------------------------\n",
      "82 participant\n",
      "[(u'82', u'NUM'), (u'participants', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'participants', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'82', u'NUM'), (u'participants', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'82', u'NUM'), (u'participants', u'NOUN')]\n",
      "------------------------------------------\n",
      "the preliminary date\n",
      "[(u'The', u'DET'), (u'preliminary', u'ADJ'), (u'dates', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'preliminary', u'ADJ'), (u'dates', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'The', u'DET'), (u'preliminary', u'ADJ'), (u'dates', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'preliminary', u'ADJ'), (u'dates', u'NOUN')]\n",
      "------------------------------------------\n",
      "6 trip\n",
      "[(u'6', u'NUM'), (u'trips', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'trips', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'6', u'NUM'), (u'trips', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'6', u'NUM'), (u'trips', u'NOUN')]\n",
      "------------------------------------------\n",
      "the program cost\n",
      "[(u'The', u'DET'), (u'program', u'NOUN'), (u'cost', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'program', u'NOUN'), (u'cost', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'The', u'DET'), (u'program', u'NOUN'), (u'cost', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'program', u'NOUN'), (u'cost', u'NOUN')]\n",
      "------------------------------------------\n",
      "person\n",
      "[(u'person', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'person', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'person', u'NOUN')]\n",
      "------------------------------------------\n",
      "luxurious private home\n",
      "[(u'luxurious', u'ADJ'), (u'private', u'ADJ'), (u'homes', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'luxurious', u'ADJ'), (u'private', u'ADJ'), (u'homes', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'luxurious', u'ADJ'), (u'private', u'ADJ'), (u'homes', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'luxurious', u'ADJ'), (u'private', u'ADJ'), (u'homes', u'NOUN')]\n",
      "------------------------------------------\n",
      "the \n",
      " premier holden road/borders road area\n",
      "[(u'the', u'DET'), (u'\\n', u'SPACE'), (u'premier', u'NOUN'), (u'Holden', u'PROPN'), (u'Road/Borders', u'PROPN'), (u'Road', u'PROPN'), (u'area', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'premier', u'NOUN'), (u'holden', u'PROPN'), (u'road/borders', u'PROPN'), (u'road', u'PROPN'), (u'area', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'\\n', u'SPACE'), (u'premier', u'NOUN'), (u'Holden', u'PROPN'), (u'Road/Borders', u'PROPN'), (u'Road', u'PROPN'), (u'area', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'\\n', u'SPACE'), (u'premier', u'NOUN'), (u'holden', u'PROPN'), (u'road/borders', u'PROPN'), (u'road', u'PROPN'), (u'area', u'NOUN')]\n",
      "------------------------------------------\n",
      "daily maid service\n",
      "[(u'daily', u'ADJ'), (u'maid', u'NOUN'), (u'service', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'daily', u'ADJ'), (u'maid', u'NOUN'), (u'service', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'daily', u'ADJ'), (u'maid', u'NOUN'), (u'service', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'daily', u'ADJ'), (u'maid', u'NOUN'), (u'service', u'NOUN')]\n",
      "------------------------------------------\n",
      "line\n",
      "[(u'line', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'line', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'line', u'NOUN')]\n",
      "------------------------------------------\n",
      "on - line registration\n",
      "[(u'on', u'ADP'), (u'-', u'PUNCT'), (u'line', u'NOUN'), (u'registration', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'on', u'ADP'), (u'-', u'PUNCT'), (u'line', u'NOUN'), (u'registration', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'on', u'ADP'), (u'-', u'PUNCT'), (u'line', u'NOUN'), (u'registration', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'on', u'ADP'), (u'-', u'PUNCT'), (u'line', u'NOUN'), (u'registration', u'NOUN')]\n",
      "------------------------------------------\n",
      "airport\n",
      "[(u'airport', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'airport', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'airport', u'NOUN')]\n",
      "------------------------------------------\n",
      "dinner\n",
      "[(u'dinner', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'dinner', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'dinner', u'NOUN')]\n",
      "------------------------------------------\n",
      "site\n",
      "[(u'site', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'site', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'site', u'NOUN')]\n",
      "------------------------------------------\n",
      "each house\n",
      "[(u'each', u'DET'), (u'house', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'house', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'each', u'DET'), (u'house', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'each', u'DET'), (u'house', u'NOUN')]\n",
      "------------------------------------------\n",
      "house\n",
      "[(u'house', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'house', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'house', u'NOUN')]\n",
      "------------------------------------------\n",
      "experienced in - house massage therapist\n",
      "[(u'experienced', u'ADJ'), (u'in', u'ADP'), (u'-', u'PUNCT'), (u'house', u'NOUN'), (u'massage', u'NOUN'), (u'therapists', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'experienced', u'ADJ'), (u'-', u'PUNCT'), (u'house', u'NOUN'), (u'massage', u'NOUN'), (u'therapists', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'experienced', u'ADJ'), (u'in', u'ADP'), (u'-', u'PUNCT'), (u'house', u'NOUN'), (u'massage', u'NOUN'), (u'therapists', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'experienced', u'ADJ'), (u'in', u'ADP'), (u'-', u'PUNCT'), (u'house', u'NOUN'), (u'massage', u'NOUN'), (u'therapists', u'NOUN')]\n",
      "------------------------------------------\n",
      "site operation\n",
      "[(u'site', u'NOUN'), (u'operation', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'site', u'NOUN'), (u'operation', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'site', u'NOUN'), (u'operation', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'site', u'NOUN'), (u'operation', u'NOUN')]\n",
      "------------------------------------------\n",
      "the 2001 ski program\n",
      "[(u'the', u'DET'), (u'2001', u'NUM'), (u'ski', u'NOUN'), (u'program', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'ski', u'NOUN'), (u'program', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'2001', u'NUM'), (u'ski', u'NOUN'), (u'program', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'2001', u'NUM'), (u'ski', u'NOUN'), (u'program', u'NOUN')]\n",
      "------------------------------------------\n",
      "an email   request\n",
      "[(u'an', u'DET'), (u'email', u'NOUN'), (u' ', u'SPACE'), (u'request', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'email', u'NOUN'), (u'request', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'an', u'DET'), (u'email', u'NOUN'), (u' ', u'SPACE'), (u'request', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'an', u'DET'), (u'email', u'NOUN'), (u' ', u'SPACE'), (u'request', u'NOUN')]\n",
      "------------------------------------------\n",
      "the \n",
      " follow information\n",
      "[(u'the', u'DET'), (u'\\n', u'SPACE'), (u'following', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'following', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'the', u'DET'), (u'\\n', u'SPACE'), (u'following', u'VERB'), (u'information', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'the', u'DET'), (u'\\n', u'SPACE'), (u'following', u'VERB')]\n",
      "------------------------------------------\n",
      "name \n",
      " company \n",
      " department \n",
      " telephone \n",
      " fax \n",
      " number\n",
      "[(u'Name', u'NOUN'), (u'\\n', u'SPACE'), (u'Company', u'PROPN'), (u'\\n', u'SPACE'), (u'Department', u'PROPN'), (u'\\n', u'SPACE'), (u'Telephone', u'PROPN'), (u'\\n', u'SPACE'), (u'Fax', u'NOUN'), (u'\\n', u'SPACE'), (u'Number', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'name', u'NOUN'), (u'company', u'PROPN'), (u'department', u'PROPN'), (u'telephone', u'PROPN'), (u'fax', u'NOUN'), (u'number', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'Name', u'NOUN'), (u'\\n', u'SPACE'), (u'Company', u'PROPN'), (u'\\n', u'SPACE'), (u'Department', u'PROPN'), (u'\\n', u'SPACE'), (u'Telephone', u'PROPN'), (u'\\n', u'SPACE'), (u'Fax', u'NOUN'), (u'\\n', u'SPACE'), (u'Number', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'name', u'NOUN'), (u'\\n', u'SPACE'), (u'company', u'PROPN'), (u'\\n', u'SPACE'), (u'department', u'PROPN'), (u'\\n', u'SPACE'), (u'telephone', u'PROPN'), (u'\\n', u'SPACE'), (u'fax', u'NOUN'), (u'\\n', u'SPACE'), (u'number', u'NOUN')]\n",
      "------------------------------------------\n",
      "people\n",
      "[(u'People', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'people', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'people', u'NOUN')]\n",
      "------------------------------------------\n",
      "choice\n",
      "[(u'Choice', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'choice', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'choice', u'NOUN')]\n",
      "------------------------------------------\n",
      "date\n",
      "[(u'Dates', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'dates', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'dates', u'NOUN')]\n",
      "------------------------------------------\n",
      "december 11th\n",
      "[(u'December', u'PROPN'), (u'11th', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'december', u'PROPN'), (u'11th', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'December', u'PROPN'), (u'11th', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'december', u'PROPN'), (u'11th', u'NOUN')]\n",
      "------------------------------------------\n",
      "all request\n",
      "[(u'all', u'DET'), (u'requests', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'requests', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'all', u'DET'), (u'requests', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'all', u'DET'), (u'requests', u'NOUN')]\n",
      "------------------------------------------\n",
      "review\n",
      "[(u'review', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'review', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords []\n",
      "------------------------------------------\n",
      "approval\n",
      "[(u'approval', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'approval', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'approval', u'NOUN')]\n",
      "------------------------------------------\n",
      "question\n",
      "[(u'Questions', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'questions', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun []\n",
      "preprocess_np_drop_typicalcommonwords [(u'questions', u'NOUN')]\n",
      "------------------------------------------\n",
      "my attention\n",
      "[(u'my', u'ADJ'), (u'attention', u'NOUN')]\n",
      "preprocess_np_drop_unwanted [(u'attention', u'NOUN')]\n",
      "preprocess_np_drop_singletermcommonnoun [(u'my', u'ADJ'), (u'attention', u'NOUN')]\n",
      "preprocess_np_drop_typicalcommonwords [(u'my', u'ADJ'), (u'attention', u'NOUN')]\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "Dorie ORG [u'NNP']\n",
      "the years DATE [u'DT', u'NNS']\n",
      "Enron ORG [u'NNP']\n",
      "Dorie Hitchcock PERSON [u'NNP', u'NNP']\n",
      "Sylvia S Pollan/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Robyn Zivic/NA/Enron@Enron PERSON [u'NNP', u'NNP']\n",
      "Andrew H PERSON [u'NNP', u'NNP']\n",
      "Geoff Storey/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Martin Cuilla/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Patrice L Mims/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Kevin Ruscitti/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Kelli PERSON [u'NNP']\n",
      "Tom Donohoe/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Joe Parks/Corp/Enron@ENRON PERSON [u'NNP', u'NNP']\n",
      "Janet H Wallis/HOU/ECT@ECT PERSON [u'NNP', u'NN', u'NNP']\n",
      "Gary Bryan/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jill T PERSON [u'NNP', u'NNP']\n",
      "Brian Bierbach/NA/Enron@Enron PERSON [u'NNP', u'NNP']\n",
      "John Craig PERSON [u'NNP', u'NNP']\n",
      "Gary Hickerson/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Daniel Reck/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "George McClellan/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Mark Tawney/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jere C PERSON [u'NNP', u'NNP']\n",
      "Larry Lawyer/NA/Enron@Enron PERSON [u'NNP', u'NNP']\n",
      "Eric Gonzales/LON/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Cindy Skinner/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "John L PERSON [u'NNP', u'NNP']\n",
      "Don Schroeder/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Doug Leach/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Bryan \n",
      "Burnett/HOU/ECT@ECT PERSON [u'NNP', u'SP', u'NNP']\n",
      "Wendy King/Corp/Enron@ENRON PERSON [u'NNP', u'NNP']\n",
      "Tim Battaglia@Enron PERSON [u'NNP', u'NNP']\n",
      "\n",
      "Douglas PERSON [u'SP', u'NNP']\n",
      "Rodney Malcolm/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Billy \n",
      "Lemmons/Corp/Enron@ENRON PERSON [u'NNP', u'SP', u'NNP']\n",
      "David Howe/Corp/Enron@ENRON PERSON [u'NNP', u'NNP']\n",
      "Mark PERSON [u'NNP']\n",
      "Frevert/NA/Enron@Enron PERSON [u'NNP']\n",
      "John J Lavorato/Corp/Enron@Enron PERSON [u'NNP', u'NNP', u'NNP']\n",
      "David W PERSON [u'NNP', u'NNP']\n",
      "Mike McConnell/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jeffrey A PERSON [u'NNP', u'NNP']\n",
      "Shankman/HOU/ECT@ECT ORG [u'NNP']\n",
      "Jeffrey McMahon/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Raymond Bowen/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jeffery Ader/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Phillip K Allen/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "John \n",
      "Arnold/HOU/ECT@ECT PERSON [u'NNP', u'SP', u'NNP']\n",
      "Edward D Baughman/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Sally Beck/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Tim Belden/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Christopher F Calger/PDX/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Wes \n",
      "Colwell/HOU/ECT@ECT PERSON [u'NNP', u'SP', u'NNP']\n",
      "Derek Davies/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Mark Dana Davis/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Joseph Deffner/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Paul Devries/TOR/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Janet R PERSON [u'NNP', u'NNP']\n",
      "Jeff Donahue/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Stephen H PERSON [u'NNP', u'NNP']\n",
      "David Duran/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Chris H Foster/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Mark E Haedicke/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Rogers Herndon/HOU/ECT@ect PERSON [u'NNP', u'NNP']\n",
      "Scott \n",
      "PERSON [u'NNP', u'SP']\n",
      "John Thompson/Corp/Enron@ENRON PERSON [u'NNP', u'NNP']\n",
      "Fred \n",
      "PERSON [u'NNP', u'SP']\n",
      "Eric LeDain/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Laura Luce/Corp/Enron@Enron PERSON [u'NNP', u'NNP']\n",
      "Thomas A Martin/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Jonathan McKay/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Michael L PERSON [u'NNP', u'NNP']\n",
      "Rob Milnthorp/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jean Mrha/NA/Enron@Enron PERSON [u'NNP', u'NNP']\n",
      "\n",
      "Scott Neal/HOU/ECT@ECT PERSON [u'SP', u'NNP', u'NNP']\n",
      "David Oxley/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Ozzie Pagan/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Beth Perlman/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Kevin M Presto/HOU/ECT@ECT PERSON [u'NNP', u'NNP', u'NNP']\n",
      "Brian \n",
      "Redmond/HOU/ECT@ECT PERSON [u'NNP', u'SP', u'NNP']\n",
      "Hunter S Shively/HOU/ECT@ECT ORG [u'NNP', u'NNP', u'NNP']\n",
      "James D PERSON [u'NNP', u'NNP']\n",
      "Bruce PERSON [u'NNP']\n",
      "Sukaly/Corp/Enron@Enron PERSON [u'NNP']\n",
      "Mike Swerzbin/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Scott \n",
      "PERSON [u'NNP', u'SP']\n",
      "Barry Tycholiz/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Frank W PERSON [u'NNP', u'NNP']\n",
      "Greg Wolfe/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Max Yzaguirre/NA/Enron@ENRON PERSON [u'NNP', u'NNP']\n",
      "John Zufferli/CAL/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Deirdre McCaffrey/HOU/ECT@ECT PERSON [u'NNP', u'NNP']\n",
      "Jennifer \n",
      "PERSON [u'NNP', u'SP']\n",
      "2001 DATE [u'CD']\n",
      "Winter PERSON [u'NNP']\n",
      "Customer Ski Program ORG [u'NNP', u'NNP', u'NNP']\n",
      "2001 DATE [u'CD']\n",
      "Beaver Creek GPE [u'NNP', u'NNP']\n",
      "Colorado GPE [u'NNP']\n",
      " February 21 - March 14 EVENT [u'SP', u'NNP', u'CD', u':', u'NNP', u'CD']\n",
      "2,600 MONEY [u'CD']\n",
      "6 CARDINAL [u'CD']\n",
      "February 21 - Sat DATE [u'NNP', u'CD', u'HYPH', u'NNP']\n",
      "February 24 DATE [u'NNP', u'CD']\n",
      "2 CARDINAL [u'CD']\n",
      "Sun GPE [u'NNP']\n",
      "February 25 DATE [u'NNP', u'CD']\n",
      "February 28 DATE [u'NNP', u'CD']\n",
      "February 28 - Sat DATE [u'NNP', u'CD', u'HYPH', u'NNP']\n",
      "March 3 DATE [u'NNP', u'CD']\n",
      "March 4 - Wed DATE [u'NNP', u'CD', u'HYPH', u'NNP']\n",
      "March 7 DATE [u'NNP', u'CD']\n",
      "March 7 - Sat DATE [u'NNP', u'CD', u'HYPH', u'NNP']\n",
      "March 10 DATE [u'NNP', u'CD']\n",
      "March 11 - Wed DATE [u'NNP', u'CD', u'HYPH', u'NNP']\n",
      "March 14 DATE [u'NNP', u'CD']\n",
      "Holden Road/Borders PERSON [u'NNP', u'NNP']\n",
      "Beaver GPE [u'NNP']\n",
      "two CARDINAL [u'CD']\n",
      "One CARDINAL [u'CD']\n",
      "Enron ORG [u'NNP']\n",
      "2001 DATE [u'CD']\n",
      "Friday DATE [u'NNP']\n",
      "December 8th DATE [u'NNP', u'NN']\n",
      "2nd & 3rd ORG [u'NNP', u'CC', u'NNP']\n",
      "The week of December DATE [u'DT', u'NN', u'IN', u'NNP']\n",
      "713 CARDINAL [u'CD']\n",
      "853 CARDINAL [u'CD']\n"
     ]
    }
   ],
   "source": [
    "def np_drop_singletermcommonnoun(noun_chunk):\n",
    "    tokens = [(token.orth_, token.pos_) for token in noun_chunk]\n",
    "    if(len(tokens) == 1 and tokens[0][1] == 'NOUN'):\n",
    "        return []\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def np_drop_unwanted(noun_chunk):\n",
    "    unwanted = ['SPACE', 'DET', 'NUM']\n",
    "    stop_words = set('for a of the and to in what my you this is your where'.split())\n",
    "    tokens = [(token.orth_.lower(), token.pos_) for token in noun_chunk \n",
    "                  if token.pos_ not in unwanted and token.orth_ not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def np_drop_typicalcommonwords(noun_chunk):\n",
    "    common_words = set('information meeting review'.split())\n",
    "    tokens = [(token.orth_.lower(), token.pos_) for token in noun_chunk \n",
    "                  if token.orth_ not in common_words]\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "    \n",
    "doc = nlp(unicode(content, \"utf-8\"))\n",
    "nchunks = doc.noun_chunks\n",
    "print 'Candidate Keyphrases'\n",
    "for nc in nchunks:\n",
    "    print nc.lemma_\n",
    "    print [(token.orth_, token.pos_) for token in nc]\n",
    "    print 'preprocess_np_drop_unwanted', np_drop_unwanted(nc)\n",
    "    print 'preprocess_np_drop_singletermcommonnoun', np_drop_singletermcommonnoun(nc)\n",
    "    print 'preprocess_np_drop_typicalcommonwords', np_drop_typicalcommonwords(nc)\n",
    "    print '------------------------------------------'\n",
    "#     for token in nc:\n",
    "#         print token.orth_, token.pos_\n",
    "#         print 'xxxxxxxxxxxx'\n",
    "    \n",
    "print '-----------------------------'\n",
    "# nounphrases = [[np.orth_, np.root.head.orth_] for np in doc.noun_chunks]\n",
    "# print 'nounphrases with head', nounphrases\n",
    "# print '-----------------------------'\n",
    "# print 'Named Entities'\n",
    "entities3 = doc.ents\n",
    "for e in entities3:\n",
    "    print e.text, e.label_ , [w.tag_ for w in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_keyphrases_by_tfidf\n",
      "generate_candidate_phrases\n",
      "texts input to dictionary [[u'guys'], [u'california', u'issues'], [u'everything'], [u'position'], [u'very', u'effective', u'manner'], [u'our', u'view'], [u'growth'], [u'iso', u'structures'], [u'necessary', u'evil'], [u'socio-political', u'leaning'], [u'these', u'entities'], [u'directors'], [u'these', u'boards'], [u'full', u'court', u'press'], [u'plan'], [u'place']]\n",
      "tfidf of the corpus\n",
      "growth 1.0\n",
      "everything 1.0\n",
      "place 1.0\n",
      "plan 1.0\n",
      "guys 1.0\n",
      "directors 1.0\n",
      "position 1.0\n",
      "entities 0.8\n",
      "boards 0.8\n",
      "socio-political 0.707106781187\n",
      "our 0.707106781187\n",
      "issues 0.707106781187\n",
      "structures 0.707106781187\n",
      "evil 0.707106781187\n",
      "california 0.707106781187\n",
      "necessary 0.707106781187\n",
      "iso 0.707106781187\n",
      "leaning 0.707106781187\n",
      "view 0.707106781187\n",
      "these 0.6\n",
      "manner 0.57735026919\n",
      "court 0.57735026919\n",
      "full 0.57735026919\n",
      "very 0.57735026919\n",
      "press 0.57735026919\n",
      "effective 0.57735026919\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from collections import defaultdict, OrderedDict\n",
    "# from pprints import pprint # pretty-printer\n",
    "doc = nlp(unicode(content, \"utf-8\"))\n",
    "nchunks = doc.noun_chunks\n",
    "def generate_candidate_phrases():\n",
    "    print 'generate_candidate_phrases'\n",
    "    for nc in nchunks:\n",
    "#         print nc.text.lower()\n",
    "        yield nc.text.lower()\n",
    "def score_keyphrases_by_tfidf(candidate_phrases):\n",
    "    print 'score_keyphrases_by_tfidf'\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    candidate_phrases = list(generate_candidate_phrases())\n",
    "    texts = [[word.lower() for word in candidate_phrase.split() if word.lower() not in stoplist]\n",
    "             for candidate_phrase in candidate_phrases]\n",
    "#     frequency = defaultdict(int)\n",
    "#     for text in texts:\n",
    "#         for token in text:\n",
    "#             frequency[token] += 1\n",
    "#     texts2 = [[token for token in text ]\n",
    "#                  for text in texts]\n",
    "    print 'texts input to dictionary', texts\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(aCandidatePhrase.split()) for aCandidatePhrase in candidate_phrases]\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf, dictionary\n",
    "\n",
    "doctfidf, docdictionary = score_keyphrases_by_tfidf(nchunks)\n",
    "print 'tfidf of the corpus'\n",
    "\n",
    "d = {docdictionary.get(id): value for doc in doctfidf for id, value in doc}\n",
    "d_in_order = sorted(d.items(), key=lambda t: t[1], reverse=True)\n",
    "# print d_in_order\n",
    "for (name, value) in d_in_order:\n",
    "    print name, value\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_candidate_phrases\n",
      "texts [[u'guys'], [u'california', u'issues'], [u'everything'], [u'position'], [u'very', u'effective', u'manner'], [u'view'], [u'growth'], [u'iso', u'structures'], [u'necessary', u'evil'], [u'socio-political', u'leaning'], [u'these', u'entities'], [u'directors'], [u'these', u'boards'], [u'full', u'court', u'press'], [u'plan'], [u'place']]\n",
      "words [u'guys', u'california', u'issues', u'everything', u'position', u'very', u'effective', u'manner', u'view', u'growth', u'iso', u'structures', u'necessary', u'evil', u'socio-political', u'leaning', u'these', u'entities', u'directors', u'these', u'boards', u'full', u'court', u'press', u'plan', u'place']\n",
      "add edge for [u'california', u'guys']\n",
      "add edge for [u'california', u'issues']\n",
      "add edge for [u'everything', u'issues']\n",
      "add edge for [u'everything', u'position']\n",
      "add edge for [u'position', u'very']\n",
      "add edge for [u'effective', u'very']\n",
      "add edge for [u'effective', u'manner']\n",
      "add edge for [u'manner', u'view']\n",
      "add edge for [u'growth', u'view']\n",
      "add edge for [u'growth', u'iso']\n",
      "add edge for [u'iso', u'structures']\n",
      "add edge for [u'necessary', u'structures']\n",
      "add edge for [u'evil', u'necessary']\n",
      "add edge for [u'evil', u'socio-political']\n",
      "add edge for [u'leaning', u'socio-political']\n",
      "add edge for [u'leaning', u'these']\n",
      "add edge for [u'entities', u'these']\n",
      "add edge for [u'directors', u'entities']\n",
      "add edge for [u'directors', u'these']\n",
      "add edge for [u'boards', u'these']\n",
      "add edge for [u'boards', u'full']\n",
      "add edge for [u'court', u'full']\n",
      "add edge for [u'court', u'press']\n",
      "add edge for [u'plan', u'press']\n",
      "add edge for [u'place', u'plan']\n",
      "calculated n_keywords 3\n",
      "keywords set([u'these', u'california', u'plan'])\n",
      "scored keyphrases are....\n",
      "these : 0.0670361809298\n",
      "california : 0.04634080451\n",
      "plan : 0.0458641059252\n"
     ]
    }
   ],
   "source": [
    "from itertools import takewhile, tee, izip\n",
    "import networkx\n",
    "\n",
    "doc = nlp(unicode(content, \"utf-8\"))\n",
    "nchunks = doc.noun_chunks\n",
    "def generate_candidate_phrases():\n",
    "    print 'generate_candidate_phrases'\n",
    "    for nc in nchunks:\n",
    "#         print nc.text.lower()\n",
    "        yield nc.text.lower()\n",
    "    \n",
    "def score_keyphrases_by_textrank(text, n_keywords=0.1):\n",
    "    \n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "#     words = [word.lower()\n",
    "#              for sent in nltk.sent_tokenize(text)\n",
    "#              for word in nltk.word_tokenize(sent)]\n",
    "    stoplist = 'for a of the and to in'.split()\n",
    "    pronoun_list = 'what where this that which what whose his her our their'.split()\n",
    "    candidate_phrases = generate_candidate_phrases()\n",
    "    \n",
    "    texts = [[word.lower() for word in candidate_phrase.split() if word.lower() not in set(stoplist+pronoun_list)]\n",
    "             for candidate_phrase in candidate_phrases]\n",
    "    print 'texts', texts\n",
    "    words = [word.lower() for phrase in texts\n",
    "                  for word in phrase]\n",
    "    print 'words', words\n",
    "    \n",
    "    \n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(words))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return izip(a, b)\n",
    "    for w1, w2 in pairwise(words):\n",
    "        if w2:\n",
    "            print 'add edge for', sorted([w1, w2])\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(words) * n_keywords))\n",
    "        print 'calculated n_keywords', n_keywords\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    print 'keywords', keywords\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.iteritems(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "scored_keyphrases = score_keyphrases_by_textrank('junk')\n",
    "print 'scored keyphrases are....'\n",
    "for kw,score in scored_keyphrases:\n",
    "    print kw, ':', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
