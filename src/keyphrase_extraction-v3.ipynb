{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileFPath='/home/mprakash/Projects/enron-experiments/significant-terms/output/firstpass/maildir/delainey-d/maildir_delainey-d_all_documents_428.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janet, get rid of these LM's - too many cooks!\n",
      "Regards\n",
      "Delainey\n",
      "\tEnron North America Corp.\n",
      "\t\n",
      "\tFrom:  Tammy R Shepperd                           08/08/2000 03:17 PM\n",
      "\t\n",
      "\n",
      "To: David W Delainey/HOU/ECT@ECT\n",
      "cc:  \n",
      "Subject: Master Turbine Lists\n",
      "\n",
      "As I mentioned to you earlier, there is much interest/concern about managing \n",
      "the ENE portfolio of turbines, both from a balance sheet and a deal \n",
      "perspective, as a result I've had numerous data requests in the last few \n",
      "weeks including one today from RAC.\n",
      "\n",
      "I thought you might be interested in the comment I have highlighted below, in \n",
      "blue, regarding the LM6000's.\n",
      "\n",
      "Tammy\n",
      "\n",
      "\n",
      "\n",
      "Fred L Kelly@ENRON_DEVELOPMENT\n",
      "08/03/2000 03:09 PM\n",
      "To: Mike Coleman/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT\n",
      "cc: Dick Westfahl/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT, fkelly@flash.net.com, \n",
      "Dan Shultz/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT, Matthew \n",
      "Tezyk/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT, Bill Williams@ECT, David \n",
      "Parquet/SF/ECT@ECT, Wayne Mays@ECT, Mathew Gimble@ECT, Chip \n",
      "Schneider/NA/Enron@Enron, Rick L Carson@ECT, Stephanie McGinnis@ECT, David \n",
      "Gorte@ECT \n",
      "Subject: Re: Master Turbine Listing  \n",
      "\n",
      "Thanks Mike - the effort underway (by RAC and EECC) is an overall Enron \n",
      "turbine effort, to which the LM6000's are a subset.  Rick Buy must report to \n",
      "Skilling and the board on the \"long\" turbine position....... thus the current \n",
      "effort.  RAC and EECC have initated a more formal process where RAC will be \n",
      "keeping a master list of turbines for Enron (like I am currently doing) with \n",
      "input from EECC.  Dan Shultz's team will be plugged into that effort so we \n",
      "don't have redundant list efforts going.  \n",
      "\n",
      "Matt Gimble's list looks very detailed and specific to the LM6000 effort - I \n",
      "think some of his data will be benefitial to the Enron Master Turbine List.  \n",
      "I will ask that Mat Tezyk and Rick Carson/RAC to coordinate.  \n",
      "\n",
      "Additionally - I think Dan/Mat Tezyk have some turbine lists they are using \n",
      "to track manufacturing and QA checks........, I don't think those lists are \n",
      "intended to ever replace the Enron Master Turbine list.  \n",
      "\n",
      "Finally - Larry Izzo asked that Dan put a list together regarding the \n",
      "LM6000's so that he (Izzo) could interface with ESA, ENA, and RAC.  I'm sure \n",
      "that Dan could use Matt Gimble's list, overlaying ESA's needs (if they were \n",
      "not listed).  \n",
      "\n",
      "In summary - If we have redundant efforts going - help me highlight them so \n",
      "we can remove unneccessary work.  \n",
      "\n",
      "Thanks for the note!  F.Kelly\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mike Coleman\n",
      "08/03/2000 12:16 PM\n",
      "To: Dick Westfahl, Fred Kelly, Dan \n",
      "Shultz/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT, Mall Tezyk, Bill Williams, Dave \n",
      "Parquet, Wayne Mays\n",
      "cc: Mathew Gimble@ECT \n",
      "\n",
      "Subject: Master Turbine Listing\n",
      "\n",
      "To avoid confusion, we should not have everyone generating separate turbine \n",
      "lists.  Matt Gimble at ENA/Houston is sheperding a comprehensive list and is \n",
      "setting it up at a site that everyone can access.  The current issue is \n",
      "attached.  When Matt has the site location set up, I will notify you.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "Mike\n",
      "\n",
      "Mathew Gimble@ECT\n",
      "08/03/2000 11:25 AM\n",
      "To: Mike Coleman/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT\n",
      "cc:  \n",
      "\n",
      "Subject: Master Turbine Listing\n",
      "\n",
      "Mike,\n",
      "\n",
      "The Master Turbine Listing contains the schedules that Tammy Shepperd and I \n",
      "gathered together.\n",
      "It includes an overal Turbine Inventory, Brian Hulse's LM6000 schedules and \n",
      "an LM6000 Timing page.\n",
      "The LM6000 Configuration is Ben Jacoby's schedule.\n",
      "\n",
      "As soon as Tammy and I set up the protection on the Master Turbine Schedule, \n",
      "I will send you its\n",
      "location.\n",
      "\n",
      "Thanks for all of your help and feedback,\n",
      "Mathew\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(fileFPath, 'r')\n",
    "content = f.read()\n",
    "f.close()\n",
    "# print 'File Content'\n",
    "print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import *\n",
    "\n",
    "set_noise = set('information meeting review group thanks regards'.split())\n",
    "dict_orgs = {}\n",
    "dict_people = {}\n",
    "dict_location = {}\n",
    "dict_vocab = {}\n",
    "\n",
    "noise_words = set('information meeting review group thanks regards additional informational to cc bcc am pm enron'.split())\n",
    "stop_words = set('for a of the and to in what my you this is your where what our'.split())\n",
    "unwanted_pos = ['SPACE', 'DET', 'NUM', 'PRON']\n",
    "\n",
    "\n",
    "def drop_as_candidate(noun_chunk):\n",
    "    # drop when the noun phrase is a single word, and is a pronoun, determiner or other common noise\n",
    "   \n",
    "#     tokens = [(token.orth_, token.pos_) for token in noun_chunk]\n",
    "    tokens = [token for token in noun_chunk]\n",
    "    if(len(tokens) == 1):\n",
    "#         print 'token is', tokens[0][0], tokens[0][1]\n",
    "        if(tokens[0].pos_ in unwanted_pos): return True\n",
    "        if(tokens[0].orth_ in stop_words): return True\n",
    "        if(tokens[0].orth_ in noise_words): return True\n",
    "        return False\n",
    "    else:\n",
    "        # if the chunk has any space in POS, it is probably not useful and we can drop it from consideration\n",
    "#         space_token = [token for token in noun_chunk if token.pos_ == 'SPACE']\n",
    "#         return len(space_token) > 0\n",
    "        return False\n",
    "\n",
    "def drop_as_candidate2(tokens_of_a_chunk):\n",
    "    # drop when the noun phrase is a single word, and is a pronoun, determiner or other common noise\n",
    "   \n",
    "#     tokens = [(token.orth_, token.pos_) for token in noun_chunk]\n",
    "#     tokens = [token for token in noun_chunk]\n",
    "    if(len(tokens_of_a_chunk) == 1):\n",
    "#         print 'token is', tokens[0][0], tokens[0][1]\n",
    "        if(tokens_of_a_chunk[0].pos_ in unwanted_pos): return True\n",
    "        if(tokens_of_a_chunk[0].orth_.lower() in stop_words): return True \n",
    "        if(tokens_of_a_chunk[0].orth_.lower() in noise_words): return True\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "\n",
    "def convert_multiline_phrase_to_singleline(noun_chunk):\n",
    "    singleline_tokens = [token for token in noun_chunk if token.pos_ != 'SPACE']\n",
    "    return singleline_tokens\n",
    "\n",
    "def convert_multiline_phrase_to_multiple_phrases(noun_chunk):\n",
    "    '''\n",
    "Original text looks like this-> \n",
    "\n",
    "PUHCA Stand-Alone \n",
    "\n",
    "Dave -\n",
    "\n",
    "FYI.\n",
    "\n",
    "\n",
    "Raw Tokens look like this -> [(u'PUHCA', u'PROPN'), (u'Stand', u'VERB'), (u'-', u'PUNCT'), (u'Alone', u'PROPN'), (u'\\n\\n', u'SPACE'), (u'Dave', u'PROPN'), (u'-', u'PUNCT'), (u'\\n\\n', u'SPACE'), (u'FYI.', u'PROPN')]\n",
    "\n",
    "Converted tokens (showing here only the words, not the pos tag)> \n",
    "[u'PUHCA', u'Stand', u'-', u'Alone']\n",
    "Converted> [u'Dave', u'-']\n",
    "Converted> [u'FYI.']\n",
    "\n",
    "    '''\n",
    "    multiple_phrases = []\n",
    "    current_phrase = []\n",
    "    for token in noun_chunk:\n",
    "        if token.pos_ != 'SPACE':\n",
    "            current_phrase.append(token)\n",
    "        else:\n",
    "            multiple_phrases.append(current_phrase)\n",
    "            current_phrase = []\n",
    "    multiple_phrases.append(current_phrase)\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "#         current_phrase.append(token) if token.pos_ != 'SPACE' else multiple_phrases.append(current_phrase) current_phrase = []\n",
    "    return multiple_phrases\n",
    "\n",
    "def replace_numbers_and_dates(tokens):\n",
    "#     modified_tokens = [token for token in tokens \n",
    "#                   if token.pos_ not in unwanted_pos and token.orth_.lower() not in stop_words]\n",
    "    \n",
    "    modified_tokens = [token for token in tokens if token.pos_ not in ['NUM']]\n",
    "    return modified_tokens\n",
    "    \n",
    "\n",
    "\n",
    "def compose_phrase_from_tokens(tokens):\n",
    "#     X_NO_SPACE = ''\n",
    "#     X_SINGLE_SPACE = ' '\n",
    "#     token_string_as_list = [(lambda x: X_NO_SPACE if x.pos_=='PUNCT' else X_SINGLE_SPACE)(token) + token.orth_ for token in tokens]\n",
    "    token_string_as_list = [token.orth_ if token.pos_ in ['PROPN','NOUN'] else token.orth_.lower() for token in tokens]\n",
    "    # print 'token string as list->', token_string_as_list\n",
    "    # if the last token is punctuation, like dash or dot - . then ignore them\n",
    "    if(token_string_as_list[-1] in ['-','.']):\n",
    "        token_string_as_list = token_string_as_list[:-1]\n",
    "    \n",
    "    result_phrase = ' '.join(token_string_as_list)\n",
    "    # get rid of leading/trailing space or punctuation chars if any\n",
    "    result_phrase = result_phrase.strip(' ,.-')\n",
    "    return result_phrase\n",
    "    \n",
    "\n",
    "def remove_unbalanced_quote(phrase):\n",
    "    corrected_phrase = phrase\n",
    "    lastsinglequoteindices = [index for index, eachchar in enumerate(phrase) if eachchar=='\\'']\n",
    "    lastdoublequoteindices = [index for index, eachchar in enumerate(phrase) if eachchar=='\"']\n",
    "    \n",
    "#     if (len(lastsinglequoteindices) % 2 == 1):\n",
    "#         # this is unbalanced, remove the last quote\n",
    "        \n",
    "#         # find position to remove\n",
    "#         pos = lastsinglequoteindices[-1]\n",
    "        \n",
    "#         # remove the character at that position\n",
    "#         corrected_phrase = phrase[:pos] + phrase[(pos+1):] \n",
    "        \n",
    "    if (len(lastdoublequoteindices) % 2 == 1):\n",
    "        # this is unbalanced, remove the last quote\n",
    "        \n",
    "        # find position to remove\n",
    "        pos2 = lastdoublequoteindices[-1]\n",
    "        \n",
    "        # remove the character at that position\n",
    "        corrected_phrase = corrected_phrase[:pos2] + corrected_phrase[(pos2+1):] \n",
    "    \n",
    "    return corrected_phrase\n",
    "\n",
    "def split_commas_into_additional_phrases(phrase):\n",
    "    subphrases = phrase.split(\",\")\n",
    "    return [phrase.strip() for phrase in subphrases]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def generate_candidate_chunks(doc):\n",
    "    nchunks = doc.noun_chunks\n",
    "    candidate_chunks = []\n",
    "\n",
    "    for nc in nchunks:\n",
    "        print 'RAW_CHUNK->', [(token.orth_, token.pos_) for token in nc]\n",
    "    #     print 'RAW_CHUNK_END'\n",
    "        rectified_tokens1 = convert_multiline_phrase_to_singleline(nc)\n",
    "        drop_this_chunk = drop_as_candidate(nc)\n",
    "        if( drop_this_chunk ):\n",
    "            print 'PROCESSED_CHUNK->', 'Dropped'\n",
    "            continue\n",
    "        else:\n",
    "            rectified_tokens2 = replace_numbers_and_dates(rectified_tokens1)\n",
    "            reconstructed_phrase = compose_phrase_from_tokens(rectified_tokens2)\n",
    "            reconstructed_phrase2 = remove_unbalanced_quote(reconstructed_phrase)\n",
    "            multi_phrase_by_comma_split = split_commas_into_additional_phrases(reconstructed_phrase2)\n",
    "#             print 'PROCESSED_CHUNK->', compose_phrase_from_tokens(rectified_tokens)\n",
    "\n",
    "            candidate_chunks.extend(multi_phrase_by_comma_split)\n",
    "        \n",
    "    return candidate_chunks\n",
    "\n",
    "def generate_candidate_chunks2(doc):\n",
    "    nchunks = doc.noun_chunks\n",
    "    candidate_chunks = []\n",
    "    # each chunk is made up of set of spacy tokens\n",
    "    # were going to see if a chunk itself should better be broken into multiple chunks\n",
    "    # \n",
    "    tokens_for_chunks = []\n",
    "    \n",
    "    for nc in nchunks:\n",
    "        # print 'Original->', nc\n",
    "        sub_phrases = convert_multiline_phrase_to_multiple_phrases(nc)\n",
    "        # print 'Expanded->', sub_phrases\n",
    "        tokens_for_chunks.extend(sub_phrases)\n",
    "    \n",
    "\n",
    "    for xchunk in tokens_for_chunks:\n",
    "        # print 'RAW_CHUNK->', [(xtoken.orth_, xtoken.pos_) for xtoken in xchunk]\n",
    "    #     print 'RAW_CHUNK_END'\n",
    "        \n",
    "        drop_this_chunk = drop_as_candidate2(xchunk)\n",
    "        if( drop_this_chunk ):\n",
    "            # print 'PROCESSED_CHUNK->', 'Dropped'\n",
    "            continue\n",
    "        else:\n",
    "            rectified_tokens2 = replace_numbers_and_dates(xchunk)\n",
    "            if(drop_as_candidate2(rectified_tokens2)):\n",
    "                # print 'PROCESSED_CHUNK->', 'Dropped'\n",
    "                continue\n",
    "            else:\n",
    "                reconstructed_phrase = compose_phrase_from_tokens(rectified_tokens2)\n",
    "                reconstructed_phrase2 = remove_unbalanced_quote(reconstructed_phrase)\n",
    "                multi_phrase_by_comma_split = split_commas_into_additional_phrases(reconstructed_phrase2)\n",
    "                # print 'PROCESSED_CHUNK->', compose_phrase_from_tokens(rectified_tokens)\n",
    "                candidate_chunks.extend(multi_phrase_by_comma_split)\n",
    "        \n",
    "    return candidate_chunks\n",
    "\n",
    "def test_chunk_creation_if_space_separated(doc):\n",
    "    nchunks = doc.noun_chunks\n",
    "    candidate_chunks = []\n",
    "    for nc in nchunks:\n",
    "        print 'Original->', nc\n",
    "        print 'RAW->',[(token.orth_, token.pos_) for token in nc]\n",
    "        multiple_phrases = convert_multiline_phrase_to_multiple_phrases(nc)\n",
    "        for p in multiple_phrases:\n",
    "            print 'Converted>', [token.orth_ for token in p]\n",
    "        \n",
    "    \n",
    "\n",
    "def examine_sentence_parsing(doc):\n",
    "    sents = doc.sents\n",
    "    for sent in sents:\n",
    "        print 'Sentence->', sent\n",
    "    \n",
    "    \n",
    "def generate_org_entities(doc):\n",
    "    entities = doc.ents\n",
    "    org_entities = [e.text for e in entities if e.label_=='ORG']\n",
    "    for xorg in org_entities:\n",
    "        # print e.text, e.label_ , [w.tag_ for w in e]\n",
    "        sanitized_xorg = xorg.strip(',.- ')\n",
    "        print 'ORG->', sanitized_xorg\n",
    "\n",
    "    \n",
    "\n",
    "doc = nlp(unicode(content, \"utf-8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncomment to see parsed sentences\n"
     ]
    }
   ],
   "source": [
    "# examine_sentence_parsing(doc)\n",
    "print 'uncomment to see parsed sentences'\n",
    "# test_chunk_creation_if_space_separated(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these LM 's - too many cooks\n",
      "Delainey\n",
      "Enron North America Corp\n",
      "Tammy R Shepperd\n",
      "David W Delainey/HOU/ECT@ECT\n",
      "the ENE portfolio\n",
      "turbines\n",
      "a balance sheet\n",
      "a deal\n",
      "perspective\n",
      "a result\n",
      "numerous data requests\n",
      "the last few\n",
      "weeks\n",
      "RAC\n",
      "the comment\n",
      "the LM6000\n",
      "Tammy\n",
      "Fred L Kelly@ENRON_DEVELOPMENT\n",
      "Mike Coleman/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT\n",
      "Subject\n",
      "Master Turbine\n",
      "the effort\n",
      "RAC\n",
      "an overall Enron\n",
      "turbine effort\n",
      "the LM6000\n",
      "a subset\n",
      "Rick Buy\n",
      "Skilling\n",
      "the board\n",
      "the \" long \" turbine position\n",
      "thus the current\n",
      "effort\n",
      "RAC\n",
      "a more formal process\n",
      "RAC\n",
      "a master list\n",
      "turbines\n",
      "input\n",
      "EECC\n",
      "Dan Shultz 's team\n",
      "that effort\n",
      "redundant list efforts\n",
      "Matt Gimble 's list\n",
      "the LM6000 effort\n",
      "his data\n",
      "the Enron Master Turbine List\n",
      "Mat Tezyk\n",
      "dan/mat Tezyk\n",
      "some turbine lists\n",
      "manufacturing and QA checks\n",
      "those lists\n",
      "the Enron Master Turbine list\n",
      "finally - Larry Izzo\n",
      "Dan\n",
      "a list\n",
      "ESA\n",
      "Dan\n",
      "Matt Gimble 's list\n",
      "ESA 's needs\n",
      "summary\n",
      "redundant efforts\n",
      "unneccessary work\n",
      "the note\n",
      "f.kelly\n",
      "Mike Coleman\n",
      "Dick Westfahl\n",
      "Subject\n",
      "confusion\n",
      "everyone\n",
      "separate turbine\n",
      "lists\n",
      "Matt Gimble\n",
      "ENA/Houston\n",
      "a comprehensive list\n",
      "a site\n",
      "everyone\n",
      "the current issue\n",
      "Matt\n",
      "the site location\n",
      "Mike\n",
      "Mathew Gimble@ECT\n",
      "Mike Coleman/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT\n",
      "Mike\n",
      "the Master Turbine Listing\n",
      "the schedules\n",
      "Tammy Shepperd\n",
      "an overal Turbine Inventory\n",
      "Brian Hulse 's LM6000 schedules\n",
      "an LM6000 Timing page\n",
      "the LM6000 Configuration\n",
      "Ben Jacoby 's schedule\n",
      "Tammy\n",
      "the protection\n",
      "the Master Turbine Schedule\n",
      "its\n",
      "location\n",
      "your help\n",
      "feedback\n"
     ]
    }
   ],
   "source": [
    "candidate_chunks = generate_candidate_chunks2(doc)\n",
    "for chunk in candidate_chunks:\n",
    "    print chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG-> LM\n",
      "ORG-> Enron\n",
      "ORG-> ENE\n",
      "ORG-> RAC\n",
      "ORG-> Master Turbine Listing  \n",
      "\n",
      "\n",
      "ORG-> RAC\n",
      "ORG-> EECC\n",
      "ORG-> Enron\n",
      "ORG-> LM6000\n",
      "ORG-> EECC\n",
      "ORG-> RAC\n",
      "ORG-> Enron\n",
      "ORG-> EECC\n",
      "ORG-> Enron Master Turbine List\n",
      "ORG-> Tezyk\n",
      "ORG-> the Enron Master Turbine\n",
      "ORG-> ESA\n",
      "ORG-> ENA\n",
      "ORG-> RAC\n",
      "ORG-> ESA\n",
      "ORG-> Master Turbine Listing\n",
      "ORG-> ENA/Houston\n",
      "ORG-> The Master Turbine Listing\n",
      "ORG-> LM6000\n",
      "ORG-> LM6000 Timing page\n",
      "ORG-> the Master Turbine Schedule\n"
     ]
    }
   ],
   "source": [
    "# test entities detection\n",
    "generate_org_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(doc):\n",
    "    label_candidate = generate_candidate_chunks2(doc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
