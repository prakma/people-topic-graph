{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok. topic extraction can begin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "print 'ok. topic extraction can begin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 3), (3, 4), (4, 5)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import networkx\n",
    "\n",
    "\n",
    "def browse_recursive(folder_path):\n",
    "    fileList = glob.glob(folder_path+'/*')\n",
    "    print 'no of files', str(len(fileList))\n",
    "    errorlist = []\n",
    "    for f in fileList:\n",
    "        yield f\n",
    "\n",
    "def get_file_content(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    return content\n",
    "\n",
    "def parse_file_content(content):\n",
    "    doc = nlp(unicode(content, \"utf-8\"))\n",
    "    return doc\n",
    "\n",
    "def get_named_entities(doc):\n",
    "    entities3 = doc.ents\n",
    "    for e in entities3:\n",
    "        print e.text, e.label_ , [w.tag_ for w in e]\n",
    "\n",
    "def get_candidate_keyphrases(doc):\n",
    "#     nchunks = doc.noun_chunks\n",
    "#     print 'Candidate Keyphrases'\n",
    "#     for nc in nchunks:\n",
    "#         print nc\n",
    "    \n",
    "    nounphrases = [np.orth_ for np in doc.noun_chunks]\n",
    "    return nounphrases\n",
    "\n",
    "def np_drop_singletermcommonnoun(noun_chunk):\n",
    "    tokens = [(token.orth_, token.pos_) for token in noun_chunk]\n",
    "    if(len(tokens) == 1 and tokens[0][1] == 'NOUN'):\n",
    "        return []\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def np_drop_unwanted(noun_chunk):\n",
    "    unwanted = ['SPACE', 'DET', 'NUM']\n",
    "    stop_words = set('for a of the and to in what my you this is your where'.split())\n",
    "    tokens = [(token.orth_.lower(), token.pos_) for token in noun_chunk \n",
    "                  if token.pos_ not in unwanted and token.orth_ not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def np_drop_typicalcommonwords(noun_chunk):\n",
    "    common_words = set('information meeting review'.split())\n",
    "    tokens = [(token.orth_.lower(), token.pos_) for token in noun_chunk \n",
    "                  if token.orth_ not in common_words]\n",
    "    return tokens\n",
    "    \n",
    "def create_graph():\n",
    "    graph = networkx.Graph()\n",
    "    return graph\n",
    "\n",
    "def add_candidate_phrases_as_graph_edges(all_doc_phrases, graph):\n",
    "    all_doc_phrases1 = iter(all_doc_phrases)\n",
    "    all_doc_phrases2 = itertools.islice(all_doc_phrases1, 1, None)\n",
    "    sequential_pairs = itertools.izip(all_doc_phrases1, all_doc_phrases2)\n",
    "    for phrase1, phrase2 in sequential_pairs:\n",
    "        graph.add_edge(phrase1, phrase2)\n",
    "\n",
    "def save_graph(graph, output_file):\n",
    "    networkx.write_gexf(graph, output_file )\n",
    "    \n",
    "\n",
    "def python_iter_test():\n",
    "    t = [1,2,3,4,5]\n",
    "    it1 = iter(t)\n",
    "    it2 = iter(t)\n",
    "    it2 = itertools.islice(it2, 1, None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pairs = itertools.izip(it1, it2)\n",
    "    print list(pairs)\n",
    "#     it = iter(t)\n",
    "#     print list(it)\n",
    "#     pairs = zip(t[::2], t[1::2])\n",
    "#     print pairs\n",
    "\n",
    "python_iter_test()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of files 910\n",
      "error   for file  /home/mprakash/Projects/enron-experiments/significant-terms/output/firstpass/maildir/delainey-d/maildir_delainey-d_all_documents_883.\n",
      "error   for file  /home/mprakash/Projects/enron-experiments/significant-terms/output/firstpass/maildir/delainey-d/maildir_delainey-d_all_documents_908.\n",
      "topic graph created\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    root_folder='/home/mprakash/Projects/enron-experiments/significant-terms/output/firstpass/maildir/delainey-d'\n",
    "    graph_output_file_path = '/home/mprakash/Projects/enron-experiments/significant-terms/output/delainey-topic-graph.gexf'\n",
    "    G = create_graph()\n",
    "    for f in browse_recursive(root_folder):\n",
    "        try:\n",
    "            doc = parse_file_content(get_file_content(f))\n",
    "            add_candidate_phrases_as_graph_edges(get_candidate_keyphrases(doc), G)\n",
    "#             print 'success', f\n",
    "        except Exception as ex:\n",
    "            print 'error ', ex, 'for file ', f\n",
    "    save_graph(G, graph_output_file_path)\n",
    "    print 'topic graph created'\n",
    "\n",
    "main()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
